{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d39bcf0e-0d11-48d7-8907-ddc51d186f88",
      "metadata": {
        "id": "d39bcf0e-0d11-48d7-8907-ddc51d186f88"
      },
      "source": [
        "# NLP Interview Assistant: Combining Knowledge Retrieval with LLMs\n",
        "\n",
        "## Sudip Das"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0c40af9-ee19-4ba7-939f-1d588fb90ee6",
      "metadata": {
        "id": "f0c40af9-ee19-4ba7-939f-1d588fb90ee6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "import re\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "IBm8kFY6xwxc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBm8kFY6xwxc",
        "outputId": "ce3d9ce7-06b1-43d3-d5f4-f4b603eb29a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c90b62d-8b9c-498c-aeb6-cd31e768aae6",
      "metadata": {
        "id": "3c90b62d-8b9c-498c-aeb6-cd31e768aae6",
        "outputId": "d04b9fa1-8030-4640-8848-92e120af95b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecf6dfe1-9d1a-4c1c-b133-760daddf95f1",
      "metadata": {
        "id": "ecf6dfe1-9d1a-4c1c-b133-760daddf95f1"
      },
      "source": [
        "## Data Collection\n",
        "### PyPDF (scrape content from PDFs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8ca9876-56c9-4f60-80fc-b349be02fb54",
      "metadata": {
        "id": "f8ca9876-56c9-4f60-80fc-b349be02fb54",
        "outputId": "ae625cb2-6d63-42f0-eb53-c995f9e8e710"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3eab65d-8829-43cf-b44b-e83a404389b6",
      "metadata": {
        "id": "c3eab65d-8829-43cf-b44b-e83a404389b6",
        "outputId": "b3abd9f0-6dee-4bbe-e477-d51e8426a345"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                            Question  \\\n",
            "0  Which of the following techniques can be used ...   \n",
            "1  Which of the following techniques can be used ...   \n",
            "2  What are the possible features of a text corpu...   \n",
            "3  You created a document term matrix on the inpu...   \n",
            "4  Which of the text parsing techniques can be us...   \n",
            "\n",
            "                                              Answer  \n",
            "0  a) Lemmatization helps to get to the base form...  \n",
            "1  b) and c) \\nDistance between two word vectors ...  \n",
            "2  e)All of the above can be used as features of ...  \n",
            "3                                                 d)  \n",
            "4                                                 d)  \n"
          ]
        }
      ],
      "source": [
        "# Function to clean question text\n",
        "def clean_question_text(question):\n",
        "    question = re.sub(r\"^Q\\d+\\.\\s*\", \"\", question).strip()\n",
        "    return question\n",
        "\n",
        "# Load and extract text\n",
        "file_path = \"100 NLP interview questions and answers.pdf\"\n",
        "reader = PdfReader(file_path)\n",
        "\n",
        "# Combine all text from the PDF\n",
        "all_text = \"\"\n",
        "for page in reader.pages:\n",
        "    all_text += page.extract_text()\n",
        "\n",
        "# Split text into individual Q&A\n",
        "qa_pattern = r\"Q\\d+\\..*?(?=Q\\d+\\.|$)\"\n",
        "qa_matches = re.findall(qa_pattern, all_text, re.DOTALL)\n",
        "\n",
        "# Function to clean the answer text\n",
        "def clean_answer_text(answer):\n",
        "    return re.sub(r\"^Answer\\s*:\\s*\", \"\", answer).strip()\n",
        "\n",
        "# Parse questions and answers\n",
        "qa_pairs = []\n",
        "for qa in qa_matches:\n",
        "    question_match = re.search(r\"(Q\\d+\\..*?)(Answer\\s*:.*)\", qa, re.DOTALL)\n",
        "    if question_match:\n",
        "        # Clean the question and answer\n",
        "        question = clean_question_text(question_match.group(1).strip())\n",
        "        answer = clean_answer_text(question_match.group(2).strip())      # Remove Answer\n",
        "        qa_pairs.append({\"Question\": question, \"Answer\": answer})\n",
        "\n",
        "# Create a DataFrame for structured processing\n",
        "qa_df_pdf = pd.DataFrame(qa_pairs)\n",
        "\n",
        "print(qa_df_pdf.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b90402f6-9cd4-4c51-9dc7-4905de09fb7f",
      "metadata": {
        "id": "b90402f6-9cd4-4c51-9dc7-4905de09fb7f",
        "outputId": "1108d6a5-e291-4429-fca2-cf57d0a8958c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question 1:\n",
            "Which of the following techniques can be used for keyword normalization in \n",
            "NLP, the process of converting a keyword into its base form? \n",
            "a. Lemmatization \n",
            "b. Soundex \n",
            "c. Cosine Similarity \n",
            "d. N-grams\n",
            "Answer:\n",
            "a) Lemmatization helps to get to the base form of a word, e.g. are playing -> play, ea ting \n",
            "-> eat, etc.Other options are meant for different purposes.\n",
            "--------------------------------------------------\n",
            "Question 2:\n",
            "Which of the following techniques can be used to compute the distance \n",
            "between two word vectors in NLP? \n",
            "a. Lemmatization \n",
            "b. Euclidean distance \n",
            "c. Cosine Similarity \n",
            "d. N-grams\n",
            "Answer:\n",
            "b) and c) \n",
            "Distance between two word vectors can be computed using Cosine similarity and Euclidean \n",
            "Distance.  Cosine Similarity establishes a cosine angle between the vector of two words . A cosi ne \n",
            "angle close to each other between two word vectors indicates the words are simil ar and vice a \n",
            "versa. \n",
            "E.g. cosine angle between two words “Football” and “Cricket” will be closer to 1 as co mpared to \n",
            "angle between the words “Football” and “New Delhi”\n",
            "--------------------------------------------------\n",
            "Question 3:\n",
            "What are the possible features of a text corpus in NLP? \n",
            "a. Count of the word in a document \n",
            "b. Vector notation of the word \n",
            "c. Part of Speech Tag \n",
            "d. Basic Dependency Grammar \n",
            "e. All of the above\n",
            "Answer:\n",
            "e)All of the above can be used as features of the text corpus.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Print the first 3 question-answer pairs\n",
        "for i in range(3):\n",
        "    print(f\"Question {i+1}:\")\n",
        "    print(qa_df_pdf.loc[i, \"Question\"])\n",
        "    print(\"Answer:\")\n",
        "    print(qa_df_pdf.loc[i, \"Answer\"])\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb9af60-1fc4-4166-86ab-fd71e433006d",
      "metadata": {
        "id": "fdb9af60-1fc4-4166-86ab-fd71e433006d"
      },
      "source": [
        "### Web Scraping (BeautifulSoup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef848c61-9978-4107-94ab-d0bd1c273003",
      "metadata": {
        "id": "ef848c61-9978-4107-94ab-d0bd1c273003",
        "outputId": "da91cade-7303-4f6b-f5a7-27c4f214f3c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully fetched the webpage!\n",
            "                                            Question  \\\n",
            "0                        What do you know about NLP?   \n",
            "1  Give examples of any two real-world applicatio...   \n",
            "2                       What is tokenization in NLP?   \n",
            "3  What is the difference between stemming and le...   \n",
            "4                                       What is NLU?   \n",
            "\n",
            "                                              Answer  \n",
            "0  NLP stands for Natural Language Processing. It...  \n",
            "1  1. Spelling/Grammar Checking Apps:The mobile a...  \n",
            "2  Tokenization is the process of splitting runni...  \n",
            "3  Both stemming and lemmatization are keyword no...  \n",
            "4  NLU stands for Natural Language Understanding....  \n"
          ]
        }
      ],
      "source": [
        "# URL of the webpage to scrape\n",
        "url = \"https://www.projectpro.io/article/nlp-interview-questions-and-answers/439\"\n",
        "\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    print(\"Successfully fetched the webpage!\")\n",
        "else:\n",
        "    print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
        "\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "def clean_question_text(question):\n",
        "    question = re.sub(r\"^Question \\d+:\", \"\", question).strip()\n",
        "    # Remove any numbering like \"1.\", \"2.\" at the start of the actual question\n",
        "    question = re.sub(r\"^\\d+\\.\\s*\", \"\", question).strip()\n",
        "    return question\n",
        "\n",
        "# Extract Q&A pairs\n",
        "qa_pairs = []\n",
        "questions = soup.find_all(\"p\", {\"style\": \"padding-left: 40px;\"})  # Match questions based on style\n",
        "\n",
        "for question in questions:\n",
        "    # Extract and clean question text\n",
        "    question_text = question.find(\"strong\")\n",
        "    if question_text:\n",
        "        q_text = clean_question_text(question_text.get_text(strip=True))\n",
        "\n",
        "        answer_paragraphs = []\n",
        "        sibling = question.find_next_sibling()\n",
        "        while sibling and sibling.name == \"p\" and \"padding-left: 40px;\" not in sibling.get(\"style\", \"\"):\n",
        "            if sibling.find(\"table\") is None:\n",
        "                answer_paragraphs.append(sibling.get_text(strip=True))\n",
        "            sibling = sibling.find_next_sibling()\n",
        "\n",
        "        if answer_paragraphs:\n",
        "            a_text = \" \".join(answer_paragraphs)\n",
        "            qa_pairs.append({\"Question\": q_text, \"Answer\": a_text})\n",
        "\n",
        "web_qa_df = pd.DataFrame(qa_pairs)\n",
        "\n",
        "print(web_qa_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88f5ec57-b90e-4cc4-addd-3c12b46596ef",
      "metadata": {
        "id": "88f5ec57-b90e-4cc4-addd-3c12b46596ef",
        "outputId": "5a11c9c7-fa4a-47a2-8743-ccecde5a6fa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question 1:\n",
            "What do you know about NLP?\n",
            "Answer:\n",
            "NLP stands for Natural Language Processing. It deals with making a machine understand the way human beings read and write in a language. This task is achieved by designing algorithms that can extract meaning from large datasets in audio or text format by applying machine learning algorithms.\n",
            "--------------------------------------------------\n",
            "Question 2:\n",
            "Give examples of any two real-world applications of NLP.\n",
            "Answer:\n",
            "1. Spelling/Grammar Checking Apps:The mobile applications and websites that offer users correct grammar mistakes in the entered text rely on NLP algorithms. These days, they can also recommend the following few words that the user might type, which is also because of specific NLP models being used in the backend. 2.ChatBots:Many websites now offer customer support through these virtual bots that chat with the user and resolve their problems. It acts as a filter to the issues that do not require an interaction with the companies’ customer executives. Begin Your Big Data Journey with ProjectPro's Project-BasedPySpark Online Course!\n",
            "--------------------------------------------------\n",
            "Question 3:\n",
            "What is tokenization in NLP?\n",
            "Answer:\n",
            "Tokenization is the process of splitting running text into words and sentences.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Print the first 3 questions and answers\n",
        "for i in range(3):\n",
        "    print(f\"Question {i+1}:\")\n",
        "    print(web_qa_df.loc[i, \"Question\"])\n",
        "    print(\"Answer:\")\n",
        "    print(web_qa_df.loc[i, \"Answer\"])\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fdabf06-9942-487a-83ba-2e38f6c0d286",
      "metadata": {
        "id": "0fdabf06-9942-487a-83ba-2e38f6c0d286",
        "outputId": "bfe0976e-29d6-4936-a661-1fcc6f6c10b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully fetched the webpage!\n",
            "Question 1:\n",
            "What is Natural Language Processing (NLP), and why is it important?\n",
            "Answer:\n",
            "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. It involves teaching machines to understand, interpret, and generate human language in a way that is valuable. NLP is important because it enables applications like translation services, sentiment analysis, chatbots, and more, which can process large amounts of data quickly and efficiently. It helps in automating routine tasks, improving customer service, and providing insights from unstructured data. When evaluating the candidate’s response, look for a clear and concise explanation, an understanding of practical applications, and the ability to relate NLP to real-world scenarios and benefits.\n",
            "--------------------------------------------------\n",
            "Question 2:\n",
            "Can you explain the difference between NLP and text mining?\n",
            "Answer:\n",
            "NLP and text mining are closely related but serve different purposes. NLP focuses on understanding and generating human language using computational techniques. It's about enabling machines to comprehend and respond in human language. Text mining, on the other hand, involves extracting useful information from text data. It's more about analyzing large volumes of text to find patterns, trends, or insights. An ideal candidate should be able to distinguish between the objectives and processes of NLP and text mining. Look for explanations that include examples of applications of both fields.\n",
            "--------------------------------------------------\n",
            "Question 3:\n",
            "What are some common challenges faced in NLP?\n",
            "Answer:\n",
            "Some common challenges in NLP include handling ambiguity in language, dealing with different languages and dialects, understanding context and sentiment, and managing unstructured data. Other challenges include the need for large annotated datasets for training models and ensuring privacy and ethical considerations in data usage. Candidates should mention real-world challenges they have encountered and how they addressed them. This shows practical experience and problem-solving skills.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Website 2\n",
        "url = \"https://www.adaface.com/blog/nlp-interview-questions/\"\n",
        "\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    print(\"Successfully fetched the webpage!\")\n",
        "    page_content = response.text\n",
        "else:\n",
        "    print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
        "    exit()\n",
        "\n",
        "# Parse the webpage content\n",
        "soup = BeautifulSoup(page_content, 'html.parser')\n",
        "\n",
        "qa_pairs = []\n",
        "\n",
        "# questions are in <h3> tags, checked from developer tools\n",
        "questions = soup.find_all('h3')\n",
        "\n",
        "for question in questions:\n",
        "    # Extract the question text\n",
        "    q_text = question.get_text(strip=True)\n",
        "\n",
        "    # Remove leading numbers like \"1.\", \"2.\", etc.\n",
        "    q_text = re.sub(r\"^\\d+\\.\\s*\", \"\", q_text)\n",
        "\n",
        "    # Collect all answer elements until the next <h3> tag\n",
        "    answer_elements = question.find_next_siblings()\n",
        "    answer_text = \"\"\n",
        "\n",
        "    for element in answer_elements:\n",
        "        # Stop if another question is encountered\n",
        "        if element.name == \"h3\":\n",
        "            break\n",
        "        if element.name == \"p\":\n",
        "            answer_text += element.get_text(strip=True) + \" \"\n",
        "\n",
        "    qa_pairs.append({\"Question\": q_text.strip(), \"Answer\": answer_text.strip()})\n",
        "\n",
        "qa_df = pd.DataFrame(qa_pairs)\n",
        "\n",
        "\n",
        "# Display the first 3 question-answer pairs\n",
        "for i in range(min(3, len(qa_df))):  # Ensure we don't try to access rows if fewer than 3 exist\n",
        "    print(f\"Question {i+1}:\")\n",
        "    print(qa_df.loc[i, \"Question\"])\n",
        "    print(\"Answer:\")\n",
        "    print(qa_df.loc[i, \"Answer\"])\n",
        "    print(\"-\" * 50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63b9f7d3-ecc8-4abc-ad67-905512e594a2",
      "metadata": {
        "id": "63b9f7d3-ecc8-4abc-ad67-905512e594a2"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f0858a8-e43e-4298-8033-175846501f89",
      "metadata": {
        "id": "4f0858a8-e43e-4298-8033-175846501f89"
      },
      "outputs": [],
      "source": [
        "# Combine all scraped data into a single DataFrame\n",
        "qa_df_combined = pd.concat([qa_df_pdf, web_qa_df, qa_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "effe7f2a-c071-4839-9c6e-8829d8ec3178",
      "metadata": {
        "id": "effe7f2a-c071-4839-9c6e-8829d8ec3178",
        "outputId": "6933516c-d444-4990-d002-eb8ba6b69046"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Word Count in qa_df_combined: 11959\n"
          ]
        }
      ],
      "source": [
        "# Calculate word count in the combined dataframe\n",
        "qa_df_combined['Word Count'] = qa_df_combined['Question'].apply(lambda x: len(str(x).split())) + qa_df_combined['Answer'].apply(lambda x: len(str(x).split()))\n",
        "print(f\"Total Word Count in qa_df_combined: {qa_df_combined['Word Count'].sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e3c1e36-79d8-4863-a183-ae119cd41ec1",
      "metadata": {
        "id": "1e3c1e36-79d8-4863-a183-ae119cd41ec1",
        "outputId": "d69c19e6-0b8d-4870-df4d-fb4fca3c4c5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence_transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence_transformers) (4.47.0)\n",
            "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence_transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence_transformers) (0.26.5)\n",
            "Requirement already satisfied: Pillow in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence_transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.6.85)\n",
            "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: faiss-cpu in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.9.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: faiss-gpu in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.7.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install faiss-gpu\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d20a65d-ee55-4be9-a258-f38f1718e182",
      "metadata": {
        "id": "0d20a65d-ee55-4be9-a258-f38f1718e182"
      },
      "outputs": [],
      "source": [
        "# Load a sentence transformer model to encode the questions\n",
        "os.environ[\"HF_TOKEN\"] = \"\" #add your HF token\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Lightweight embedding model\n",
        "questions = qa_df_combined[\"Question\"].tolist()\n",
        "\n",
        "# Convert questions to embeddings\n",
        "embeddings = embedding_model.encode(questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e05c00eb-fb10-49cb-bb08-cbce7867714a",
      "metadata": {
        "id": "e05c00eb-fb10-49cb-bb08-cbce7867714a"
      },
      "outputs": [],
      "source": [
        "# Create a FAISS index\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(embeddings))  # Add embeddings to the index\n",
        "\n",
        "faiss.write_index(index, \"faiss_index.bin\") #Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2221bf28-3c94-4ac5-941a-f777c3ed2f93",
      "metadata": {
        "id": "2221bf28-3c94-4ac5-941a-f777c3ed2f93"
      },
      "outputs": [],
      "source": [
        "# Retrieval Function\n",
        "def retrieve_context(query, index, questions, model, top_k=3, similarity_threshold=0.7):\n",
        "    query_embedding = model.encode([query])\n",
        "\n",
        "    # Search the FAISS index\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    # Retrieve the top-k results\n",
        "    context = []\n",
        "    for idx, dist in zip(indices[0], distances[0]):\n",
        "        if dist < similarity_threshold:\n",
        "            context.append(questions[idx])\n",
        "\n",
        "    return context if context else []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb2327e1-74a3-493e-8443-345f64dcc8f8",
      "metadata": {
        "id": "eb2327e1-74a3-493e-8443-345f64dcc8f8"
      },
      "source": [
        "## LLMs - Llama-3.2 and Flan-T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e135d3a-e855-4f33-9355-beeb4d2cd2dc",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "aef69284a17b430ca6acf42eb45e7c79"
          ]
        },
        "id": "3e135d3a-e855-4f33-9355-beeb4d2cd2dc",
        "outputId": "d0754347-7980-4518-d897-7efd862818f0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aef69284a17b430ca6acf42eb45e7c79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load Llama 3.2 model from hugging face\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
        "lm_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0b9acd9-f231-4a84-a9cb-44d6f4d79909",
      "metadata": {
        "id": "a0b9acd9-f231-4a84-a9cb-44d6f4d79909"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "# Load Flan-T5 model\n",
        "flan_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "flan_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7062ff50-192b-491d-b1e7-138babebb0ec",
      "metadata": {
        "id": "7062ff50-192b-491d-b1e7-138babebb0ec"
      },
      "outputs": [],
      "source": [
        "def generate_answer(query, context, tokenizer, model):\n",
        "\n",
        "    #corpus has mcq questions, hence needs to handle it separately if question asked later is not an mcq and matches with the original question\n",
        "    is_mcq_context = context and any(opt in '. '.join(context) for opt in [\"a.\", \"b.\", \"c.\", \"d.\"])\n",
        "    is_mcq_query = any(keyword in query.lower() for keyword in [\"choose\", \"select\", \"option\", \"correct\"])\n",
        "\n",
        "    if not context:\n",
        "        input_text = f\"Question: {query}\\n\\nProvide a brief answer:\" #use llm if context not available\n",
        "    elif is_mcq_context and is_mcq_query:\n",
        "        # Handle MCQ format only if the query is MCQ\n",
        "        input_text = f\"Context: {'. '.join(context)}\\n\\nQuestion: {query}\\n\\nProvide a concise explanation for the correct answer only, ignoring other options:\"\n",
        "    else:\n",
        "        # Handle normal question format or non-MCQ queries\n",
        "        input_text = f\"Context: {'. '.join(context)}\\n\\nQuestion: {query}\\n\\nProvide a short and accurate Answer:\"\n",
        "\n",
        "\n",
        "    # Tokenize and generate\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "    outputs = model.generate(**inputs, max_length=400, num_beams=5, early_stopping=True)\n",
        "\n",
        "    # Decode the answer\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    # Remove MCQ artifacts if the query is not explicitly an MCQ\n",
        "    if not is_mcq_query:\n",
        "        answer = re.sub(r\"^[A-Da-d]\\)\\s*\", \"\", answer)\n",
        "        # truncate the explanation for incorrect options after the correct answer\n",
        "        answer = re.split(r\"Note:|Explanation:\", answer)[0].strip()\n",
        "\n",
        "\n",
        "\n",
        "    # Generic cleanup\n",
        "    finality_patterns = [\n",
        "        r\"(?i)the best answer is.*\",\n",
        "        r\"(?i)therefore, the correct answer is.*\",\n",
        "        r\"(?i)correct answer:.*\"\n",
        "    ]\n",
        "    for pattern in finality_patterns:\n",
        "        answer = re.sub(pattern, \"\", answer)\n",
        "\n",
        "\n",
        "    # Remove the question from the answer (case-insensitive)\n",
        "    pattern = rf\"(?i)Question:\\s*{re.escape(query)}?\"\n",
        "    answer = re.sub(pattern, \"\", answer, flags=re.DOTALL)\n",
        "    # Remove context from the answer\n",
        "    context_pattern = rf\"(?i)Context:\\s*{re.escape('. '.join(context) if context else '')}?\"\n",
        "    answer = re.sub(context_pattern, \"\", answer, flags=re.DOTALL)\n",
        "\n",
        "    # Remove generic headers like \"Context\" or \"Provide a brief answer\"\n",
        "    answer = re.sub(r\"(?i)context:.*?Question:.*?\", \"\", answer, flags=re.DOTALL)\n",
        "    answer = re.sub(r\"(?i)provide a brief answer:?\", \"\", answer, flags=re.DOTALL)\n",
        "    answer = re.sub(r\"(?i)Provide a concise explanation for the correct answer only, ignoring other optionsr*?:\", \"\", answer)\n",
        "    answer = re.sub(r\"(?i)Provide a short and accurate Answer*?:\", \"\", answer)\n",
        "    answer = answer.split(\"Answer:\", 1)[-1].strip()\n",
        "\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb91e87f-6d3e-455b-8cb8-0aef9facfb91",
      "metadata": {
        "id": "cb91e87f-6d3e-455b-8cb8-0aef9facfb91",
        "outputId": "575b3602-3ac3-4c0d-a942-2dcf190310de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What are stop words, and why are they removed in NLP?\n",
            "Retrieved Context: ['What are stop words, and why are they removed in text processing?', 'What are stop words?', 'In NLP, The process of removing words like “and”, “is”, “a”, “an”, “the” from \\na sentence is called as \\na. Stemming \\nb. Lemmatization \\nc. Stop word \\nd. All of the above']\n",
            "--------------------------------------------------------------------------------\n",
            "Generated Answer: Stop words are common words like “and”, “is”, “a”, “an”, “the” that do not add much value to the meaning of a sentence. They are removed in NLP to reduce the dimensionality of the text data and improve the performance of NLP tasks like text classification, sentiment analysis, and information retrieval.\n"
          ]
        }
      ],
      "source": [
        "# Test model\n",
        "query = \"What are stop words, and why are they removed in NLP?\"\n",
        "retrieved_context = retrieve_context(query, index, questions, embedding_model)\n",
        "generated_answer = generate_answer(query, retrieved_context, tokenizer, lm_model)\n",
        "\n",
        "print(f\"Question: {query}\")\n",
        "print(f\"Retrieved Context: {retrieved_context}\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Generated Answer: {generated_answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83790732-c9bc-4300-ae3b-e718b5207de4",
      "metadata": {
        "id": "83790732-c9bc-4300-ae3b-e718b5207de4"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d579c04-0071-4469-8a82-a1bf257f02db",
      "metadata": {
        "id": "3d579c04-0071-4469-8a82-a1bf257f02db"
      },
      "outputs": [],
      "source": [
        "# List of random 30 common questions in NLP\n",
        "q = [\n",
        "    \"What is Natural Language Processing (NLP)?\",\n",
        "    \"Explain the difference between NLP and text mining.\",\n",
        "    \"What are stop words, and why are they removed in NLP?\",\n",
        "    \"What is tokenization in NLP?\",\n",
        "    \"How can I prepare for a Natural Language Processing (NLP) interview?\",\n",
        "    \"Which technique can be used for noun phrase detection in NLP?\",\n",
        "    \"What is the purpose of Part-of-Speech (POS) tagging in NLP?\",\n",
        "    \"How does named entity recognition (NER) work, and what are its applications?\",\n",
        "    \"Define Dependency Parsing and its importance in NLP.\",\n",
        "    \"What is the Bag-of-Words (BoW) model?\",\n",
        "    \"How is TF-IDF used for text representation in NLP?\",\n",
        "    \"What is word embedding, and how is it different from one-hot encoding?\",\n",
        "    \"Explain the Skip-Gram model in Word2Vec.\",\n",
        "    \"How does Transfer Learning improve NLP model performance?\",\n",
        "    \"What are the advantages of using pre-trained models in NLP?\",\n",
        "    \"How does a Transformer model work in NLP?\",\n",
        "    \"What is the significance of the attention mechanism in NLP models?\",\n",
        "    \"How do BERT and GPT differ in their approach to NLP tasks?\",\n",
        "    \"What is a language model, and why is it important in NLP?\",\n",
        "    \"Explain the concept of fine-tuning a pre-trained model for NLP tasks.\",\n",
        "    \"How is sentiment analysis performed using NLP techniques?\",\n",
        "    \"What are some real-world applications of NLP in healthcare?\",\n",
        "    \"How does machine translation work in NLP?\",\n",
        "    \"What is text summarization, and how can it be implemented in NLP?\",\n",
        "    \"How does question answering work in NLP systems?\",\n",
        "    \"What are some common challenges in text preprocessing for NLP?\",\n",
        "    \"How does ambiguity in human language affect NLP systems?\",\n",
        "    \"What are the limitations of current NLP models?\",\n",
        "    \"How does NLP handle multilingual text?\",\n",
        "    \"How can NLP models be evaluated for performance and accuracy?\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25d6af00-0b14-493e-9421-10c7bdb75525",
      "metadata": {
        "id": "25d6af00-0b14-493e-9421-10c7bdb75525"
      },
      "outputs": [],
      "source": [
        "# Function to store the outputs of the 30 questions\n",
        "def store_answers(queries, filename, tokenizer, lm_model):\n",
        "    results = []\n",
        "    for query in q:\n",
        "        retrieved_context = retrieve_context(query, index, questions, embedding_model)\n",
        "        generated_answer = generate_answer(query, retrieved_context, tokenizer, lm_model)\n",
        "        print(f\"Question: {query}\")\n",
        "        #print(f\"Retrieved Context: {retrieved_context}\")\n",
        "        print(f\"Generated Answer: {generated_answer}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        results.append({\n",
        "            \"Question\": query,\n",
        "            \"Generated Answer\": generated_answer\n",
        "        })\n",
        "\n",
        "    # Save results to a JSON file\n",
        "    with open(filename, \"w\") as f:\n",
        "        import json\n",
        "        json.dump(results, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c66538a0-10ce-420a-a12e-45d48158d0e0",
      "metadata": {
        "id": "c66538a0-10ce-420a-a12e-45d48158d0e0"
      },
      "source": [
        "### Testing Llama 3.2 response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcb56d1f-b478-4304-83e8-b2327ebde190",
      "metadata": {
        "id": "fcb56d1f-b478-4304-83e8-b2327ebde190",
        "outputId": "1dcfd77f-5bbd-40ff-8f35-b8f3a0a26c73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is Natural Language Processing (NLP)?\n",
            "Generated Answer: Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) that deals with the interaction between computers and humans in natural language. It involves the development of algorithms and statistical models that enable computers to process, understand, and generate natural language data. NLP is a crucial component of many applications, including speech recognition, language translation, sentiment analysis, and text summarization.\n",
            "\n",
            "Common elements of NLP:\n",
            "\n",
            "1.  **Tokenization**: Breaking down text into individual words or tokens.\n",
            "2.  **Stopword removal**: Removing common words like \"the,\" \"and,\" and \"a\" that do not add much value to the meaning of the text.\n",
            "3.  **Stemming or Lemmatization**: Reducing words to their base form (e.g., \"running\" becomes \"run\").\n",
            "4.  **Part-of-speech tagging**: Identifying the grammatical category of each word (e.g., noun, verb, adjective).\n",
            "5.  **Named entity recognition**: Identifying specific entities like names, locations, and organizations.\n",
            "6.  **Dependency parsing**: Analyzing the grammatical structure of a sentence.\n",
            "7.  **Semantic role labeling**: Identifying the roles played by entities in a sentence (e.g., \"Who did what to whom?\").\n",
            "8.  **Sentiment analysis**: Determining the emotional tone or attitude conveyed by text.\n",
            "9.  **Text classification**: Categorizing text into predefined categories (e.g., spam vs. non-spam emails).\n",
            "10. **Language modeling**: Predicting the next word in a\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Explain the difference between NLP and text mining.\n",
            "Generated Answer: NLP (Natural Language Processing) is a subfield of artificial intelligence that deals with the interaction between computers and humans in natural language. It involves the development of algorithms and statistical models that enable computers to process, understand, and generate natural language data. Text mining, on the other hand, is a subset of NLP that focuses specifically on extracting insights and patterns from large volumes of unstructured text data. In other words, NLP is a broader field that encompasses text mining, among other tasks such as sentiment analysis, language translation, and speech recognition.\n",
            "\n",
            "Key Points:\n",
            "\n",
            "*   NLP is a subfield of AI that deals with human-computer interaction in natural language.\n",
            "*   Text mining is a subset of NLP that focuses on extracting insights from unstructured text data.\n",
            "*   NLP encompasses a range of tasks beyond text mining, including sentiment analysis, language translation, and speech recognition.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What are stop words, and why are they removed in NLP?\n",
            "Generated Answer: Stop words are common words like “and”, “is”, “a”, “an”, “the” that do not add much value to the meaning of a sentence. They are removed in NLP to reduce the dimensionality of the text data and improve the performance of NLP tasks like text classification, sentiment analysis, and information retrieval.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is tokenization in NLP?\n",
            "Generated Answer: Tokenization in NLP refers to the process of breaking down text into individual units called tokens. These tokens can be words, subwords, or even characters, depending on the specific approach used. Tokenization is a fundamental step in natural language processing (NLP) tasks, such as text classification, sentiment analysis, and language modeling.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: How can I prepare for a Natural Language Processing (NLP) interview?\n",
            "Generated Answer: To prepare for a Natural Language Processing (NLP) interview, focus on the following areas:\n",
            "\n",
            "* Review the fundamentals of NLP, including text processing, sentiment analysis, and machine learning algorithms.\n",
            "* Familiarize yourself with popular NLP libraries and tools, such as NLTK, spaCy, and Stanford CoreNLP.\n",
            "* Practice solving common NLP problems, such as named entity recognition, part-of-speech tagging, and text classification.\n",
            "* Study real-world applications of NLP, including chatbots, sentiment analysis, and language translation.\n",
            "* Brush up on your programming skills, particularly in languages like Python, Java, or C++.\n",
            "\n",
            "By focusing on these areas, you'll be well-prepared to tackle NLP-related questions and demonstrate your expertise to potential employers.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Which technique can be used for noun phrase detection in NLP?\n",
            "Generated Answer: a. Part of speech tagging\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the purpose of Part-of-Speech (POS) tagging in NLP?\n",
            "Generated Answer: The primary purpose of Part-of-Speech (POS) tagging in Natural Language Processing (NLP) is to identify the grammatical category of each word in a sentence, such as noun, verb, adjective, adverb, etc. This helps in understanding the meaning and context of the sentence, and is a crucial step in many NLP tasks, including text classification, sentiment analysis, and machine translation.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: How does named entity recognition (NER) work, and what are its applications?\n",
            "Generated Answer: Named entity recognition (NER) is a subtask of natural language processing (NLP) that involves identifying and categorizing named entities in unstructured text into predefined categories, such as person, organization, location, and date. NER is important because it enables machines to extract relevant information from text data, which can be used for various applications, such as information retrieval, text summarization, and sentiment analysis.\n",
            "\n",
            "To approach the task of NER using machine learning, one would typically follow these steps:\n",
            "\n",
            "1.  **Data Collection**: Gather a large dataset of labeled text examples, where each example includes the text and the corresponding entity types (e.g., person, organization, location).\n",
            "2.  **Data Preprocessing**: Preprocess the text data by tokenizing it, removing stop words, and converting all text to lowercase.\n",
            "3.  **Feature Extraction**: Extract relevant features from the preprocessed text data, such as part-of-speech tags, named entity tags, and word embeddings.\n",
            "4.  **Model Training**: Train a machine learning model using the extracted features and labeled data.\n",
            "5.  **Model Evaluation**: Evaluate the performance of the trained model using metrics such as precision, recall, and F1-score.\n",
            "6.  **Model Deployment**: Deploy the trained model in a production-ready environment, where it can be used to recognize entities in new, unseen text data.\n",
            "\n",
            "Some popular machine learning algorithms for NER include:\n",
            "\n",
            "*   **Rule-based approaches**: Use predefined rules to identify entities based on their context.\n",
            "*   **Supervised learning**: Train a model using labeled data to learn the patterns and relationships between entities and their context.\n",
            "*   **Deep learning**: Use neural networks\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Define Dependency Parsing and its importance in NLP.\n",
            "Generated Answer: Dependency Parsing is a type of syntactic parsing in Natural Language Processing (NLP) that analyzes the grammatical structure of a sentence by identifying the relationships between words. It represents the sentence as a graph, where each word is a node, and the relationships between words are edges. The goal of Dependency Parsing is to identify the grammatical dependencies between words, such as subject-verb-object relationships, and to represent the sentence in a way that captures the underlying syntactic structure.\n",
            "\n",
            "Importance in NLP: Dependency Parsing is important in NLP because it can be used for a variety of tasks, such as:\n",
            "\n",
            "* Part-of-speech tagging\n",
            "* Named entity recognition\n",
            "* Sentiment analysis\n",
            "* Machine translation\n",
            "* Question answering\n",
            "\n",
            "By analyzing the grammatical structure of a sentence, Dependency Parsing can provide insights into the meaning of the sentence and the relationships between words, which can be useful for a wide range of NLP tasks.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the Bag-of-Words (BoW) model?\n",
            "Generated Answer: The Bag-of-Words (BoW) model is a simple and widely used statistical model in Natural Language Processing (NLP) that represents text as a bag, or a set, of its word frequencies, without considering the order or context of the words. This model is useful for tasks such as text classification, topic modeling, and information retrieval, as it allows for efficient and compact representation of text data.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: How is TF-IDF used for text representation in NLP?\n",
            "Generated Answer: TF-IDF is a technique used for text representation in NLP that weighs the importance of each word in a document based on its frequency and rarity across the entire corpus. It helps to reduce the impact of common words and focuses on the most informative words, leading to better performance in text classification, clustering, and information retrieval tasks.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is word embedding, and how is it different from one-hot encoding?\n",
            "Generated Answer: A word embedding is a way to represent words as vectors in a high-dimensional space, where semantically similar words are mapped to nearby points. This is different from one-hot encoding, which represents each word as a binary vector where only one element is 1, and the rest are 0. Word embeddings capture multiple dimensions of data and are represented as vectors, whereas one-hot encoding is a simple binary representation.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Explain the Skip-Gram model in Word2Vec.\n",
            "Generated Answer: The Skip-Gram model predicts the context words that are most likely to appear in the same sentence as the target word.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: How does Transfer Learning improve NLP model performance?\n",
            "Generated Answer: Transfer learning improves NLP model performance by leveraging pre-trained models on large datasets, allowing the model to learn general features and representations that can be fine-tuned for specific NLP tasks. This approach reduces the need for large amounts of labeled data and accelerates the training process.\n",
            "\n",
            "Example: BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model that has achieved state-of-the-art results in various NLP tasks. By fine-tuning BERT on a specific task, such as sentiment analysis or question answering, the model can adapt to the task-specific requirements and improve its performance.\n",
            "\n",
            "Evaluation of NLP model performance can be done using metrics such as:\n",
            "\n",
            "* Accuracy\n",
            "* F1-score\n",
            "* Precision\n",
            "* Recall\n",
            "* ROUGE score (for text generation tasks)\n",
            "* BLEU score (for machine translation tasks)\n",
            "* Perplexity (for language modeling tasks)\n",
            "\n",
            "These metrics provide a quantitative measure of the model's performance and can be used to compare the performance of different models and hyperparameters. Additionally, techniques such as cross-validation and grid search can be used to evaluate the performance of the model and select the best hyperparameters.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What are the advantages of using pre-trained models in NLP?\n",
            "Generated Answer: Pre-trained models in NLP have several advantages, including:\n",
            "\n",
            "* Improved performance on downstream tasks\n",
            "* Reduced training time and data requirements\n",
            "* Better handling of out-of-vocabulary words\n",
            "* Ability to capture complex patterns and relationships in language\n",
            "\n",
            "These advantages make pre-trained models a popular choice in many NLP applications.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: How does a Transformer model work in NLP?\n",
            "Generated Answer: A Transformer model works in NLP by using self-attention mechanisms to weigh the importance of different words in a sentence relative to each other, allowing the model to capture long-range dependencies and contextual relationships between words. This is achieved through a multi-head attention mechanism, where the model attends to all positions in the input sequence simultaneously, and a feed-forward neural network (FFNN) to transform the output. The self-attention mechanism allows the model to weigh the importance of different words in a sentence relative to each other, allowing the model to capture long-range dependencies and contextual relationships between words. This is achieved through a multi-head attention mechanism, where the model attends to all positions in the input sequence simultaneously, and a feed-forward neural network (FFNN) to transform the output. The self-attention mechanism allows the model to weigh the importance of different words in a sentence relative to each other, allowing the model to capture long-range dependencies and contextual relationships between words. This is achieved through a multi-head attention mechanism, where the model attends to all positions in the input sequence simultaneously, and a feed-forward neural network (FFNN) to transform the output. The self-attention mechanism allows the model to weigh the importance of different words in a sentence relative to each other, allowing the model to capture long-range dependencies and contextual relationships between words. This is achieved through a multi-head attention mechanism, where the model attends to all positions in the input sequence simultaneously, and a feed-forward neural network (FFNN)\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the significance of the attention mechanism in NLP models?\n",
            "Generated Answer: The attention mechanism in NLP models allows the model to focus on specific parts of the input data that are relevant for the task at hand, rather than processing the entire input simultaneously. This enables the model to selectively weigh the importance of different words or phrases in a sentence, improving its ability to capture nuanced semantic relationships and context-dependent information. By doing so, the attention mechanism enhances the model's performance on tasks such as machine translation, question answering, and text summarization.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: How do BERT and GPT differ in their approach to NLP tasks?\n",
            "Generated Answer: BERT and GPT differ in their approach to NLP tasks. BERT uses a pre-training approach to learn contextualized representations of words, while GPT uses a self-supervised learning approach to predict the next word in a sequence. BERT is typically used for question-asking and text classification tasks, while GPT is used for language generation and text-to-text tasks. BERT and GPT differ in their approach to NLP tasks. BERT uses a pre-training approach to learn contextualized representations of words, while GPT uses a self-supervised learning approach to predict the next word in a sequence. BERT is typically used for question-asking and text classification tasks, while GPT is used for language generation and text-to-text tasks.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is a language model, and why is it important in NLP?\n",
            "Generated Answer: A language model is a statistical model that predicts the next word in a sequence of text based on the context of the previous words. It is a type of Bag-of-Words model, which represents text as a collection of words and their frequencies. Language models are important in NLP because they enable computers to understand and generate human-like language, making them useful for tasks such as language translation, text summarization, and chatbots.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Explain the concept of fine-tuning a pre-trained model for NLP tasks.\n",
            "Generated Answer: Fine-tuning a pre-trained model for NLP tasks involves adjusting the model's parameters to better suit a specific task or dataset. This is done by adding a few layers on top of the pre-trained model and training the entire model from scratch on the new task. The goal is to leverage the pre-trained model's knowledge and adapt it to the new task, resulting in improved performance.\n",
            "\n",
            "Would you like me to expand on this answer?\n",
            "\n",
            "Yes, I'd be happy to provide a more detailed explanation.\n",
            "\n",
            "Here's a more detailed explanation:\n",
            "\n",
            "Fine-tuning a pre-trained model for NLP tasks involves several key steps:\n",
            "\n",
            "1. **Choosing a pre-trained model**: Select a pre-trained model that has been trained on a large corpus of text data, such as BERT, RoBERTa, or XLNet. These models have learned to represent language in a way that is useful for a wide range of NLP tasks.\n",
            "2. **Adding a few layers on top**: Add a few layers on top of the pre-trained model to create a new model that is tailored to the specific NLP task at hand. These additional layers can include classification layers, attention mechanisms, or other components that are relevant to the task.\n",
            "3. **Training from scratch**: Train the entire model from scratch on the new task, using the pre-trained model as a starting point. This involves optimizing the parameters of the additional layers to minimize the loss function on the new task.\n",
            "4. **Fine-tuning the pre-trained model**: Fine-tune the pre-trained model by adjusting its parameters to better suit the new task. This can involve adjusting the learning rate, batch size, or other hyperparameters to optimize the model's performance.\n",
            "5. **Evaluating performance**: Evaluate the performance of the fine-tuned model on the new task, using metrics such as accuracy, F1 score, or ROUGE\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: How is sentiment analysis performed using NLP techniques?\n",
            "Generated Answer: Sentiment analysis is performed using NLP techniques by analyzing the linguistic features of text data, such as part-of-speech tags, named entities, and sentiment lexicons, to determine the emotional tone or attitude conveyed by the text. Common NLP techniques used for sentiment analysis include:\n",
            "\n",
            "1.  **Text Preprocessing**: Cleaning and normalizing the text data to remove noise and irrelevant information.\n",
            "2.  **Tokenization**: Breaking down the text into individual words or tokens.\n",
            "3.  **Part-of-Speech (POS) Tagging**: Identifying the grammatical category of each word (e.g., noun, verb, adjective).\n",
            "4.  **Named Entity Recognition (NER)**: Identifying specific entities mentioned in the text (e.g., names, locations).\n",
            "5.  **Sentiment Lexicons**: Using pre-trained models or dictionaries to map words to their corresponding sentiment scores.\n",
            "6.  **Machine Learning**: Training machine learning models on labeled datasets to learn patterns and relationships between text features and sentiment labels.\n",
            "7.  **Deep Learning**: Using neural networks to learn complex patterns and relationships in text data.\n",
            "\n",
            "By combining these techniques, NLP models can accurately determine the sentiment of text data, enabling applications such as sentiment analysis for social media posts, customer feedback, and market research.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What are some real-world applications of NLP in healthcare?\n",
            "Generated Answer: Some real-world applications of NLP in healthcare include:\n",
            "\n",
            "1. **Clinical Decision Support Systems**: NLP is used to analyze patient data, medical literature, and clinical guidelines to provide healthcare professionals with relevant information to make informed decisions.\n",
            "2. **Medical Text Analysis**: NLP is used to analyze large amounts of medical text data, such as patient notes, medical records, and research papers, to extract relevant information and identify patterns.\n",
            "3. **Chatbots and Virtual Assistants**: NLP is used to power chatbots and virtual assistants that can understand and respond to patient queries, providing 24/7 support and improving patient engagement.\n",
            "4. **Sentiment Analysis**: NLP is used to analyze patient feedback and sentiment, helping healthcare organizations to identify areas for improvement and improve patient satisfaction.\n",
            "5. **Disease Diagnosis**: NLP is used to analyze medical images, such as X-rays and MRIs, to help doctors diagnose diseases more accurately.\n",
            "\n",
            "These are just a few examples of the many real-world applications of NLP in healthcare. NLP has the potential to revolutionize the way healthcare is delivered, making it more efficient, effective, and patient-centered.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: How does machine translation work in NLP?\n",
            "Generated Answer: Machine translation is a subfield of natural language processing (NLP) that uses algorithms and statistical models to translate text from one language to another. The process typically involves the following steps:\n",
            "\n",
            "1. Tokenization: Breaking down text into individual words or tokens.\n",
            "2. Part-of-speech tagging: Identifying the grammatical category of each word (e.g., noun, verb, adjective).\n",
            "3. Dependency parsing: Analyzing the grammatical structure of the sentence.\n",
            "4. Semantic role labeling: Identifying the roles played by entities in the sentence (e.g., \"Who\" did \"what\" to \"whom\").\n",
            "5. Machine translation: Using statistical models to generate a translation of the input text.\n",
            "6. Post-processing: Refining the translation to improve its quality and fluency.\n",
            "\n",
            "Machine translation can be performed using various techniques, including:\n",
            "\n",
            "* Statistical machine translation (SMT)\n",
            "* Neural machine translation (NMT)\n",
            "* Rule-based machine translation (RBT)\n",
            "\n",
            "Each technique has its strengths and weaknesses, and the choice of technique depends on the specific application and language pair.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is text summarization, and how can it be implemented in NLP?\n",
            "Generated Answer: Text summarization is the process of extracting the most important information from a large piece of text and condensing it into a shorter summary. There are two main types of text summarization:\n",
            "\n",
            "1. **Extractive Summarization**: This type of summarization involves selecting the most important sentences or phrases from the original text and combining them into a summary.\n",
            "2. **Abstractive Summarization**: This type of summarization involves generating a new summary that is not a direct copy of the original text, but rather a condensed version that captures the main ideas and key points.\n",
            "\n",
            "Both types of summarization can be implemented in Natural Language Processing (NLP) using various techniques such as machine learning, deep learning, and rule-based approaches.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: How does question answering work in NLP systems?\n",
            "Generated Answer: In NLP systems, question answering (QA) typically involves the following steps:\n",
            "\n",
            "1. **Text Preprocessing**: Cleaning and normalizing the input text to remove noise and irrelevant information.\n",
            "2. **Question Classification**: Identifying the type of question (e.g., open-ended, multiple-choice) to determine the best approach for answering.\n",
            "3. **Knowledge Retrieval**: Searching a knowledge base or database to find relevant information related to the question.\n",
            "4. **Answer Generation**: Using the retrieved information to generate a response to the question.\n",
            "5. **Post-processing**: Refining the answer through spell-checking, grammar-checking, and fluency evaluation.\n",
            "\n",
            "This process enables NLP systems to provide accurate and relevant answers to user queries.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What are some common challenges in text preprocessing for NLP?\n",
            "Generated Answer: Some common challenges in text preprocessing for NLP include:\n",
            "\n",
            "1.  **Handling Out-of-Vocabulary (OOV) words**: Words that are not present in the training data.\n",
            "2.  **Removing stop words**: Common words like \"the\", \"and\", etc. that do not add much value to the meaning of the text.\n",
            "3.  **Dealing with punctuation and special characters**: Punctuation marks and special characters that can affect the meaning of the text.\n",
            "4.  **Removing noise and irrelevant data**: Removing irrelevant or noisy data that can affect the accuracy of the model.\n",
            "5.  **Scaling and normalization**: Scaling and normalizing the data to ensure that all features are on the same scale.\n",
            "6.  **Handling missing values**: Dealing with missing values in the data.\n",
            "7.  **Text normalization**: Normalizing the text to a standard format, such as converting all text to lowercase.\n",
            "8.  **Tokenization**: Tokenizing the text into individual words or tokens.\n",
            "9.  **Stemming or Lemmatization**: Reducing words to their base form, such as \"running\" to \"run\".\n",
            "10. **Handling non-English text**: Dealing with text in languages other than English.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: How does ambiguity in human language affect NLP systems?\n",
            "Generated Answer: Ambiguity in human language can significantly impact NLP systems, as it can lead to misinterpretation and incorrect processing of text. For example, homophones (words with the same pronunciation but different meanings) and homographs (words with the same spelling but different meanings) can cause confusion. Additionally, context-dependent words and idioms can be challenging for NLP systems to understand. To mitigate these issues, NLP systems can employ techniques such as part-of-speech tagging, named entity recognition, and semantic role labeling to disambiguate language and improve accuracy.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What are the limitations of current NLP models?\n",
            "Generated Answer: Current NLP models face several limitations, including:\n",
            "\n",
            "1.  **Lack of common sense**: NLP models often struggle to understand the nuances of human language, such as idioms, sarcasm, and figurative language.\n",
            "2.  **Limited contextual understanding**: NLP models may not fully comprehend the context in which a piece of text is being used, leading to misinterpretations.\n",
            "3.  **Inability to generalize**: NLP models are often trained on specific datasets and may not generalize well to new, unseen data.\n",
            "4.  **Vulnerability to bias**: NLP models can perpetuate biases present in the training data, leading to unfair or discriminatory outcomes.\n",
            "5.  **Inadequate handling of ambiguity**: NLP models may struggle to handle ambiguous or unclear language, leading to incorrect interpretations.\n",
            "\n",
            "These limitations highlight the need for continued research and development in NLP to improve the accuracy and reliability of NLP models.\n",
            "\n",
            "Real-world applications of NLP include:\n",
            "\n",
            "1.  **Sentiment analysis**: NLP can be used to analyze customer feedback and sentiment on social media, helping businesses improve their products and services.\n",
            "2.  **Language translation**: NLP can be used to translate text from one language to another, facilitating communication across languages and cultures.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: How does NLP handle multilingual text?\n",
            "Generated Answer: ## Step 1: Understanding NLP Basics\n",
            "NLP (Natural Language Processing) is a subfield of AI that deals with the interaction between computers and humans in natural language. It involves tasks such as text processing, sentiment analysis, and machine translation.\n",
            "\n",
            "## Step 2: Handling Multilingual Text\n",
            "To handle multilingual text in NLP, several approaches can be employed. These include:\n",
            "\n",
            "- **Tokenization**: Breaking down text into individual words or tokens, regardless of language.\n",
            "- **Stemming or Lemmatization**: Reducing words to their base form to facilitate comparison across languages.\n",
            "- **Machine Translation**: Using machine learning models to translate text from one language to another.\n",
            "- **Language Detection**: Identifying the language of the text to apply the most suitable processing techniques.\n",
            "- **Multilingual Models**: Training models that can handle multiple languages simultaneously.\n",
            "\n",
            "## Step 3: Key Considerations\n",
            "When handling multilingual text, it's crucial to consider the nuances of each language, including grammar, syntax, and cultural differences. Additionally, the choice of approach depends on the specific NLP task and the languages involved.\n",
            "\n",
            "The final answer is: $\\boxed{Tokenization, Stemming or Lemmatization, Machine Translation, Language Detection, Multilingual Models}$\n",
            "--------------------------------------------------\n",
            "Question: How can NLP models be evaluated for performance and accuracy?\n",
            "Generated Answer: NLP models can be evaluated for performance and accuracy using metrics such as precision, recall, F1-score, accuracy, and ROUGE score. Additionally, techniques like cross-validation, grid search, and early stopping can be used to optimize model performance. To ensure fairness and avoid bias, it's essential to use diverse and representative datasets, employ debiasing techniques, and monitor model performance on underrepresented groups. Regular auditing and testing can also help identify and address potential biases.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "store_answers(q, \"llama_answers.json\", tokenizer, lm_model) #and display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53fae603-94be-4d72-b470-fdae18c82b4c",
      "metadata": {
        "id": "53fae603-94be-4d72-b470-fdae18c82b4c"
      },
      "source": [
        "### Testing Flan-T5 response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46de5fec-4eea-4129-9c27-91ae1523413e",
      "metadata": {
        "id": "46de5fec-4eea-4129-9c27-91ae1523413e",
        "outputId": "558eface-560d-4e99-8164-6b2dc50cb9db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is Natural Language Processing (NLP)?\n",
            "Generated Answer: Natural Language Processing (NLP) is a branch of computer science that deals with the processing of natural language.\n",
            "--------------------------------------------------\n",
            "Question: Explain the difference between NLP and text mining.\n",
            "Generated Answer: Natural Language Processing (NLP) is a branch of computer science that focuses on the processing of natural language.\n",
            "--------------------------------------------------\n",
            "Question: What are stop words, and why are they removed in NLP?\n",
            "Generated Answer: In NLP, The process of removing words like “and”, “is”, “a”, “an”, “the” from a sentence is called as a. Stemming b. Lemmatization c. Stop word d.\n",
            "--------------------------------------------------\n",
            "Question: What is tokenization in NLP?\n",
            "Generated Answer: Tokenization is the process of separating a set of words from a set of words.\n",
            "--------------------------------------------------\n",
            "Question: How can I prepare for a Natural Language Processing (NLP) interview?\n",
            "Generated Answer: You can prepare for an NLP interview by reading books on the subject.\n",
            "--------------------------------------------------\n",
            "Question: Which technique can be used for noun phrase detection in NLP?\n",
            "Generated Answer: c. Continuous Bag of Words\n",
            "--------------------------------------------------\n",
            "Question: What is the purpose of Part-of-Speech (POS) tagging in NLP?\n",
            "Generated Answer: Part-of-Speech (POS) tagging is a method of tagging parts of speech in natural language processing ( NLP ) .\n",
            "--------------------------------------------------\n",
            "Question: How does named entity recognition (NER) work, and what are its applications?\n",
            "Generated Answer: Named entity recognition (NER) is the process of recognizing the name of a given entity in a given context.\n",
            "--------------------------------------------------\n",
            "Question: Define Dependency Parsing and its importance in NLP.\n",
            "Generated Answer: Dependency Parsing Dependency Parsing (DP) is the process of parsing a data set to extract information from it.\n",
            "--------------------------------------------------\n",
            "Question: What is the Bag-of-Words (BoW) model?\n",
            "Generated Answer: The Bag-of-Words (BoW) model is a word embedding model used in NLP .\n",
            "--------------------------------------------------\n",
            "Question: How is TF-IDF used for text representation in NLP?\n",
            "Generated Answer: TF-IDF (Term Frequency-Inverse Document Frequency) is a technique used for text representation in NLP.\n",
            "--------------------------------------------------\n",
            "Question: What is word embedding, and how is it different from one-hot encoding?\n",
            "Generated Answer: Word embeddings capture multiple dimensions of data and are represented as vectors\n",
            "--------------------------------------------------\n",
            "Question: Explain the Skip-Gram model in Word2Vec.\n",
            "Generated Answer: In Word2Vec , a skip-gram model is used for word embeddings .\n",
            "--------------------------------------------------\n",
            "Question: How does Transfer Learning improve NLP model performance?\n",
            "Generated Answer: Transfer learning improves the performance of an NLP model.\n",
            "--------------------------------------------------\n",
            "Question: What are the advantages of using pre-trained models in NLP?\n",
            "Generated Answer: pre-trained models can be used in a wide variety of applications\n",
            "--------------------------------------------------\n",
            "Question: How does a Transformer model work in NLP?\n",
            "Generated Answer: a transformer model is a machine learning model\n",
            "--------------------------------------------------\n",
            "Question: What is the significance of the attention mechanism in NLP models?\n",
            "Generated Answer: Attention mechanism in neural networks for NLP tasks The attention mechanism in neural networks for NLP tasks is the mechanism by which a network learns to pay attention to a particular task.\n",
            "--------------------------------------------------\n",
            "Question: How do BERT and GPT differ in their approach to NLP tasks?\n",
            "Generated Answer: BERT is a machine learning method and GPT is a data mining method\n",
            "--------------------------------------------------\n",
            "Question: What is a language model, and why is it important in NLP?\n",
            "Generated Answer: Natural Language Processing (NLP) is a branch of computer science that deals with the processing of natural language.\n",
            "--------------------------------------------------\n",
            "Question: Explain the concept of fine-tuning a pre-trained model for NLP tasks.\n",
            "Generated Answer: Fine-tuning is the process of fine-tuning a pre-trained model for NLP tasks.\n",
            "--------------------------------------------------\n",
            "Question: How is sentiment analysis performed using NLP techniques?\n",
            "Generated Answer: Sentiment analysis can be performed using a variety of NLP techniques.\n",
            "--------------------------------------------------\n",
            "Question: What are some real-world applications of NLP in healthcare?\n",
            "Generated Answer: c. Speech Biometric d. Text Summarization\n",
            "--------------------------------------------------\n",
            "Question: How does machine translation work in NLP?\n",
            "Generated Answer: machine translation is a form of natural language processing\n",
            "--------------------------------------------------\n",
            "Question: What is text summarization, and how can it be implemented in NLP?\n",
            "Generated Answer: Text summarization (TS) is a text-to-machine translation (MT) method that can be implemented in NLP .\n",
            "--------------------------------------------------\n",
            "Question: How does question answering work in NLP systems?\n",
            "Generated Answer: a question is posed to the system and the system answers the question\n",
            "--------------------------------------------------\n",
            "Question: What are some common challenges in text preprocessing for NLP?\n",
            "Generated Answer: Text Preprocessing Text Preprocessing may refer to:\n",
            "--------------------------------------------------\n",
            "Question: How does ambiguity in human language affect NLP systems?\n",
            "Generated Answer: ambiguity in human language affects the accuracy of NLP systems\n",
            "--------------------------------------------------\n",
            "Question: What are the limitations of current NLP models?\n",
            "Generated Answer: NLP models are limited in how much information they can store and how much information they can learn.\n",
            "--------------------------------------------------\n",
            "Question: How does NLP handle multilingual text?\n",
            "Generated Answer: multilingual text processing in an NLP project\n",
            "--------------------------------------------------\n",
            "Question: How can NLP models be evaluated for performance and accuracy?\n",
            "Generated Answer: How do you ensure that your NLP model is not biased?\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "store_answers(q, \"T5_answers.json\",flan_tokenizer, flan_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1MVdJrYjxMbR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MVdJrYjxMbR",
        "outputId": "dbe2ed04-bf70-4612-a35a-5737d65505a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.9)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.6)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=15781feef968465afb914cd80b559167ad4ecefefbe7e32d29363f9573fb1204\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ZU3HMmuKxUVj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU3HMmuKxUVj",
        "outputId": "d5478562-95da-4aec-815e-7c6b05b4f918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Model Evaluation Results ===\n",
            "\n",
            "--- BLEU Scores ---\n",
            "LLaMA BLEU: {'bleu': 0.030191903818260386, 'precisions': [0.12669100279149667, 0.03911821914847633, 0.01740265390472047, 0.00963433326034596], 'brevity_penalty': 1.0, 'length_ratio': 4.583661417322834, 'translation_length': 4657, 'reference_length': 1016}\n",
            "T5 BLEU: {'bleu': 0.021804534750438788, 'precisions': [0.33405639913232105, 0.08120649651972157, 0.04239401496259352, 0.02425876010781671], 'brevity_penalty': 0.3000204754021258, 'length_ratio': 0.453740157480315, 'translation_length': 461, 'reference_length': 1016}\n",
            "\n",
            "--- ROUGE Scores ---\n",
            "LLaMA ROUGE: {'rouge1': 0.24456696186268784, 'rouge2': 0.08778528539099956, 'rougeL': 0.18828350598392585, 'rougeLsum': 0.2019726109482999}\n",
            "T5 ROUGE: {'rouge1': 0.2134161650600847, 'rouge2': 0.075212847308192, 'rougeL': 0.17982557010379158, 'rougeLsum': 0.17963872543855225}\n",
            "\n",
            "--- Summary ---\n",
            "LLaMA performed better on BLEU.\n",
            "LLaMA performed better on ROUGE-L.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import evaluate\n",
        "\n",
        "def load_answers_from_file(filepath):\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    answers = [item[\"Generated Answer\"] for item in data]\n",
        "    return answers\n",
        "\n",
        "# File paths for reference and model answer files\n",
        "reference_file = ('/content/drive/My Drive/references.json')\n",
        "llama_file = ('/content/drive/My Drive/llama_answers.json')\n",
        "t5_file = ('/content/drive/My Drive/T5_answers.json')\n",
        "\n",
        "# Load the answers\n",
        "reference_answers = load_answers_from_file(reference_file)\n",
        "llama_answers = load_answers_from_file(llama_file)\n",
        "t5_answers = load_answers_from_file(t5_file)\n",
        "\n",
        "# Validate that all lists have the same length\n",
        "if not (len(reference_answers) == len(llama_answers) == len(t5_answers)):\n",
        "    raise ValueError(\"The number of answers in reference, llama, and t5 files must match.\")\n",
        "\n",
        "# Load evaluation metrics\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "# Prepare references for BLEU (as BLEU expects a list of lists for references)\n",
        "references_for_bleu = [[ref] for ref in reference_answers]\n",
        "\n",
        "# Compute BLEU for each model\n",
        "bleu_llama = bleu.compute(predictions=llama_answers, references=references_for_bleu)\n",
        "bleu_t5 = bleu.compute(predictions=t5_answers, references=references_for_bleu)\n",
        "\n",
        "# Compute ROUGE for each model\n",
        "rouge_llama = rouge.compute(predictions=llama_answers, references=reference_answers)\n",
        "rouge_t5 = rouge.compute(predictions=t5_answers, references=reference_answers)\n",
        "\n",
        "# Print the results\n",
        "print(\"=== Model Evaluation Results ===\\n\")\n",
        "\n",
        "print(\"--- BLEU Scores ---\")\n",
        "print(f\"LLaMA BLEU: {bleu_llama}\")\n",
        "print(f\"T5 BLEU: {bleu_t5}\")\n",
        "\n",
        "print(\"\\n--- ROUGE Scores ---\")\n",
        "print(\"LLaMA ROUGE:\", rouge_llama)\n",
        "print(\"T5 ROUGE:\", rouge_t5)\n",
        "\n",
        "# Simple summary comparison based on BLEU and ROUGE-L scores\n",
        "bleu_llama_score = bleu_llama[\"bleu\"]\n",
        "bleu_t5_score = bleu_t5[\"bleu\"]\n",
        "\n",
        "rouge_llama_l = rouge_llama[\"rougeL\"]\n",
        "rouge_t5_l = rouge_t5[\"rougeL\"]\n",
        "\n",
        "print(\"\\n--- Summary ---\")\n",
        "if bleu_llama_score > bleu_t5_score:\n",
        "    print(\"LLaMA performed better on BLEU.\")\n",
        "else:\n",
        "    print(\"T5 performed better on BLEU.\")\n",
        "\n",
        "if rouge_llama_l > rouge_t5_l:\n",
        "    print(\"LLaMA performed better on ROUGE-L.\")\n",
        "else:\n",
        "    print(\"T5 performed better on ROUGE-L.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6cdf637-163f-4d69-8a82-76c877146f3e",
      "metadata": {
        "id": "e6cdf637-163f-4d69-8a82-76c877146f3e"
      },
      "source": [
        "## Test your knowledge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "101c13fd-dfda-4bea-b278-4b35b99bb1d0",
      "metadata": {
        "id": "101c13fd-dfda-4bea-b278-4b35b99bb1d0",
        "outputId": "7f053bbd-19f8-48fb-f520-d841e78befb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: In NLP, The process of converting a sentence or paragraph into tokens is \n",
            "referred to as Stemming \n",
            "a. True \n",
            "b. False\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def get_random_mcq_question(qa_df_combined):\n",
        "    # Filter rows that are likely to be MCQs (contain \"a.\", \"b.\", etc.)\n",
        "    mcq_questions = qa_df_combined[qa_df_combined['Question'].str.contains(r'\\ba\\.', regex=True, na=False)]\n",
        "\n",
        "    if mcq_questions.empty:\n",
        "        return \"No MCQ questions found in the corpus.\"\n",
        "\n",
        "    # Randomly select one MCQ\n",
        "    mcq = mcq_questions.sample(n=1).iloc[0]\n",
        "    question = mcq['Question']\n",
        "    answer = mcq['Answer']\n",
        "\n",
        "    return question, answer\n",
        "\n",
        "# Example usage\n",
        "question, answer = get_random_mcq_question(qa_df_combined)\n",
        "print(f\"Question: {question}\")  # Displays the question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f3dbd4-8820-48bd-b439-c489f3e2fab4",
      "metadata": {
        "id": "a4f3dbd4-8820-48bd-b439-c489f3e2fab4",
        "outputId": "00c6dd59-3412-44d1-cce6-6e156d90d75b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'b) The statement describes the process of tokenization and not stemming, hence it  is \\nFalse.'"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the answer\n",
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "545cc6cc-4e27-4cc2-bfe4-64fd68527e53",
      "metadata": {
        "id": "545cc6cc-4e27-4cc2-bfe4-64fd68527e53",
        "outputId": "51f34361-f6fa-40e2-c0cb-e0c721bb5ca9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What are stop words, and why are they removed in NLP?\n",
            "Retrieved Context: ['What are stop words, and why are they removed in text processing?', 'What are stop words?', 'In NLP, The process of removing words like “and”, “is”, “a”, “an”, “the” from \\na sentence is called as \\na. Stemming \\nb. Lemmatization \\nc. Stop word \\nd. All of the above']\n",
            "--------------------------------------------------------------------------------\n",
            "Generated Answer: Stop words are common words like “and”, “is”, “a”, “an”, “the” that do not add much value to the meaning of a sentence. They are removed in NLP to reduce the dimensionality of the text data and improve the performance of NLP tasks like text classification, sentiment analysis, and information retrieval.\n"
          ]
        }
      ],
      "source": [
        "# Test model\n",
        "query = \"What are stop words, and why are they removed in NLP?\"\n",
        "retrieved_context = retrieve_context(query, index, questions, embedding_model)\n",
        "generated_answer = generate_answer(query, retrieved_context, tokenizer, lm_model)\n",
        "\n",
        "print(f\"Question: {query}\")\n",
        "print(f\"Retrieved Context: {retrieved_context}\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Generated Answer: {generated_answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3618932b-dd37-4bf9-8b68-f7b0902b06eb",
      "metadata": {
        "id": "3618932b-dd37-4bf9-8b68-f7b0902b06eb"
      },
      "source": [
        "## UI using Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91e87cbf-f024-4064-ac58-befd181774d3",
      "metadata": {
        "id": "91e87cbf-f024-4064-ac58-befd181774d3",
        "outputId": "bf6bc7b1-1326-4ab3-91e3-c571697a57bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (5.8.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (4.6.2.post1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.115.5)\n",
            "Requirement already satisfied: ffmpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (1.5.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.26.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (2.10.2)\n",
            "Requirement already satisfied: pydub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.0.19)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.8.2)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.41.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.32.1)\n",
            "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio-client==1.5.1->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio-client==1.5.1->gradio) (14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "* Running on local URL:  http://127.0.0.1:7871\n",
            "* Running on public URL: https://598e98d5bb0023c320.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://598e98d5bb0023c320.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio\n",
        "import gradio as gr\n",
        "\n",
        "def chatbot(model_choice, query):\n",
        "    if model_choice == \"Llama_3.2\":\n",
        "        retrieved_context = retrieve_context(query, index, questions, embedding_model)\n",
        "        return generate_answer(query, retrieved_context, tokenizer, lm_model)\n",
        "    elif model_choice == \"Flan-T5\":\n",
        "        retrieved_context = retrieve_context(query, index, questions, embedding_model)\n",
        "        return generate_answer(query, retrieved_context, flan_tokenizer, flan_model)\n",
        "\n",
        "# Define the Gradio UI\n",
        "def gradio_ui():\n",
        "    model_choices = [\"Llama_3.2\", \"Flan-T5\"]\n",
        "\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"# NLP Interview Assistant\")\n",
        "\n",
        "        with gr.Row():\n",
        "            model_selector = gr.Dropdown(choices=model_choices, label=\"Select Model\", value=\"Llama_3.2\")\n",
        "\n",
        "        query_input = gr.Textbox(label=\"Enter your query\", placeholder=\"Type your query here...\", lines=2)\n",
        "\n",
        "        output_box = gr.Textbox(label=\"Model Output\", placeholder=\"Response will appear here\", lines=4)\n",
        "\n",
        "        submit_button = gr.Button(\"Submit\")\n",
        "\n",
        "        # Link the chatbot function to the Gradio UI\n",
        "        submit_button.click(chatbot, inputs=[model_selector, query_input], outputs=output_box)\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Create the Gradio UI\n",
        "demo = gradio_ui()\n",
        "\n",
        "# Launch the UI\n",
        "demo.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09854189-a3f3-4324-9aeb-5c443c0cda85",
      "metadata": {
        "id": "09854189-a3f3-4324-9aeb-5c443c0cda85"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
